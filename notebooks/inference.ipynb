{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "dataset_folder = \"../datasets\"\n",
    "sys.path.insert(0, \"../\")\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n",
    "import models\n",
    "import video_transforms\n",
    "import darknorm\n",
    "from loss import CELoss\n",
    "from utils.util import AverageMeter, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# using gpu 0\n",
    "width = 170\n",
    "height = 128\n",
    "input_size = 112\n",
    "length = 64\n",
    "seed = 1234\n",
    "ckpt_location = 'checkpoints/ce_EE6222_DarkNormr18_triv_norm_gic_g1.8_1234'\n",
    "if 'norm' in ckpt_location:\n",
    "    dark_std = True\n",
    "else:\n",
    "    dark_std = False\n",
    "if 'gic' in ckpt_location:\n",
    "    light = True\n",
    "    # dark_std = False\n",
    "    pass\n",
    "else:\n",
    "    light = False\n",
    "if 'g1.8' in ckpt_location:\n",
    "    gamma = 1.8\n",
    "else:\n",
    "    gamma = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(model_path, num_categories, device, multiGPUTrain=True, multiGPUTest=False):\n",
    "    model = models.__dict__['DarkNorm'](num_classes=num_categories, length=length)\n",
    "    params = torch.load(model_path, map_location=device)\n",
    "\n",
    "    if multiGPUTest:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        new_dict = {\"module.\" + k: v for k, v in params['state_dict'].items()}\n",
    "        model.load_state_dict(new_dict)\n",
    "\n",
    "    elif multiGPUTrain:\n",
    "        new_dict = {k[7:]: v for k, v in params['state_dict'].items()}\n",
    "        model_dict = model.state_dict()\n",
    "        model_dict.update(new_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    else:\n",
    "        model.load_state_dict(params['state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    lossesClassification = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top2 = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, input in enumerate(val_loader):\n",
    "            (inputs, targets) = input\n",
    "            inputs = inputs.view(-1, length, 3, input_size,\n",
    "                                 input_size).transpose(1, 2)\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            lossClassification = criterion(output, targets, epoch)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            if isinstance(output, tuple) and len(output) > 1:\n",
    "                output = output[0]\n",
    "            acc1, acc2 = accuracy(output.data, targets, topk=(1, 2))\n",
    "\n",
    "            lossesClassification.update(\n",
    "                lossClassification.data.item(), output.size(0))\n",
    "\n",
    "            top1.update(acc1.item(), output.size(0))\n",
    "            top2.update(acc2.item(), output.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        print(f'validate * * Prec@1 {top1.avg:.3f} Prec@5 {top2.avg:.3f}'\n",
    "              f'Classification Loss {lossesClassification.avg:.4f}')\n",
    "    return top1.avg, top2.avg, lossesClassification.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('../', ckpt_location, 'model_best.pth.tar')\n",
    "assert os.path.exists(model_path), 'model path not exist'\n",
    "\n",
    "data_dir = os.path.join(dataset_folder, 'EE6222_frames_test')\n",
    "extension = 'img_{0:05d}.jpg'\n",
    "val_fileName = \"test_split%d.txt\" % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = os.path.join(dataset_folder, 'settings', 'EE6222', val_fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_mean_light = [0.485, 0.456, 0.406] * 1* length\n",
    "clip_std_light = [0.229, 0.224, 0.225] * 1 * length\n",
    "clip_mean = [0.0702773, 0.06571121, 0.06437492] * 1 * length\n",
    "clip_std = [0.08475896, 0.08116068, 0.07479476] * 1 * length\n",
    "if dark_std:\n",
    "    normalize = video_transforms.Normalize(mean=clip_mean_light,\n",
    "                                           std=clip_std_light)\n",
    "else:\n",
    "    normalize = video_transforms.Normalize(mean=clip_mean,\n",
    "                                                   std=clip_std)\n",
    "val_transform = video_transforms.Compose([\n",
    "        video_transforms.CenterCrop((input_size)),\n",
    "        video_transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "criterion = CELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light: True\n",
      "Gamma: 1.8\n",
      "DarkStd: True\n"
     ]
    }
   ],
   "source": [
    "val_dataset = datasets.EE6222(root=data_dir,\n",
    "                              modality=\"rgb\",\n",
    "                              source=test_file,\n",
    "                              phase=\"val\",\n",
    "                              is_color=True,\n",
    "                              new_length=length,\n",
    "                              new_width=width,\n",
    "                              new_height=height,\n",
    "                              video_transform=val_transform,\n",
    "                              num_segments=1,\n",
    "                              gamma=gamma,\n",
    "                              method='gamma',\n",
    "                              light=light)\n",
    "print(f\"Light: {light}\")\n",
    "print(f\"Gamma: {gamma}\")\n",
    "print(f\"DarkStd: {dark_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(_):\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=16, shuffle=False,\n",
    "        num_workers=8, pin_memory=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action recognition model is loaded in 1.2427 seconds.\n"
     ]
    }
   ],
   "source": [
    "model_start_time = time.time()\n",
    "spatial_net = buildModel(model_path, 10, device)\n",
    "model_end_time = time.time()\n",
    "model_time = model_end_time - model_start_time\n",
    "print(\"Action recognition model is loaded in %4.4f seconds.\" % (model_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate * * Prec@1 60.444 Prec@5 83.111Classification Loss 1.3556\n"
     ]
    }
   ],
   "source": [
    "acc1, acc2, lossClassification = validate(val_loader, spatial_net, criterion, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
